{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOLicN8YS3uT",
        "outputId": "23fa5e48-c7b8-43f5-d495-92582077443b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 1 0 0 0 2 2 1 0 0 0 1 0 2 1 2 0 2 2 2 2 2 0 1 1 1 1 2 2 0 1 1 0 2 2 0\n",
            " 1 1 2 2 1 1 0 0 0 1 1 2 2 2 1 0 1 2 2 1 1 0 1 1 2 2 2 2 1 0 2 1 0 2 0 0 1\n",
            " 1 0 0 0 2 1 0 0 1 0 1 0 0 0 1 0 1 1 2 2 2 2 0 0 2 2]\n",
            "[1 1 2 2 0 2 0 0 2 2 2 1 1 0 2 1 0 0 1 2 2 0 2 0 2 2 2 2 1 0 2 1 1 0 2 1 0\n",
            " 2 0 0 2 2 1 1 2 0 0 1 1 2 1 0 2 1 1 2 0 1 0 2 0 1 0 2 1 1 1 1 0 1 0 1 2 2\n",
            " 2 0 0 2 0 1 0 1 1 2 1 0 2 0 1 0 1 1 0 1 2 2 0 0 2 2]\n",
            "('0.039784', '0.039805', '0.039794')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# kali\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_blobs, make_circles, load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from sklearn.metrics.cluster import homogeneity_score, completeness_score, v_measure_score\n",
        "\n",
        "\n",
        "# You can import whatever standard packages are required\n",
        "\n",
        "# full sklearn, full pytorch, pandas, matplotlib, numpy are all available\n",
        "# Ideally you do not need to pip install any other packages!\n",
        "# Avoid pip install requirement on the evaluation program side, if you use above packages and sub-packages of them, then that is fine!\n",
        "\n",
        "###### PART 1 ######\n",
        "\n",
        "def get_data_blobs(n_points=100):\n",
        "  X, y = make_blobs(\n",
        "    n_samples=n_points, n_features=2,\n",
        "    centers=3, cluster_std=0.5,\n",
        "    shuffle=True, random_state=0\n",
        "  )\n",
        "  return X,y\n",
        "\n",
        "def get_data_circles(n_points=100):\n",
        "  X, y = make_circles(\n",
        "    n_samples=n_points, shuffle=True,  \n",
        "    factor=0.3, noise=0.05, random_state=0\n",
        "  )\n",
        "  return X,y\n",
        "\n",
        "def get_data_mnist():\n",
        "  digits = load_digits()\n",
        "  X=digits.data\n",
        "  y=digits.target\n",
        "  return X,y\n",
        "\n",
        "def build_kmeans(X=None,k=10):\n",
        "  km = KMeans(\n",
        "    n_clusters=3, init='random',\n",
        "    n_init=10, max_iter=300, \n",
        "    tol=1e-04, random_state=0\n",
        "  )\n",
        "  return km\n",
        "\n",
        "def assign_kmeans(km=None,X=None):\n",
        "  y_pred = km.fit_predict(X)\n",
        "  return y_pred\n",
        "\n",
        "def compare_clusterings(ypred_1=None,ypred_2=None):\n",
        "  h = \"%.6f\"% homogeneity_score(ypred_1, ypred_2)\n",
        "  c = \"%.6f\"% completeness_score(ypred_1, ypred_2)\n",
        "  v = \"%.6f\"% v_measure_score(ypred_1, ypred_2)\n",
        "  return h,c,v\n",
        "\n",
        "X_b , y_b = get_data_blobs()\n",
        "X_c, y_c = get_data_circles()\n",
        "km = build_kmeans(X = X_b, k = 10)\n",
        "y_b_pred = assign_kmeans(km, X_b)\n",
        "print(y_b_pred)\n",
        "\n",
        "km = build_kmeans(X = X_c, k = 10)\n",
        "y_c_pred = assign_kmeans(km, X_c)\n",
        "print(y_c_pred)\n",
        "\n",
        "print(compare_clusterings(y_b_pred, y_c_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### PART 2 ######\n",
        "\n",
        "def build_lr_model(X=None, y=None):\n",
        "  pass\n",
        "  lr_model = None\n",
        "  # write your code...\n",
        "  # Build logistic regression, refer to sklearn\n",
        "  return lr_model\n",
        "\n",
        "def build_rf_model(X=None, y=None):\n",
        "  pass\n",
        "  rf_model = None\n",
        "  # write your code...\n",
        "  # Build Random Forest classifier, refer to sklearn\n",
        "  return rf_model\n",
        "\n",
        "def get_metrics(model=None,X=None,y=None):\n",
        "  pass\n",
        "  # Obtain accuracy, precision, recall, f1score, auc score - refer to sklearn metrics\n",
        "  acc, prec, rec, f1, auc = 0,0,0,0,0\n",
        "  # write your code here...\n",
        "  return acc, prec, rec, f1, auc\n",
        "\n",
        "def get_paramgrid_lr():\n",
        "  # you need to return parameter grid dictionary for use in grid search cv\n",
        "  # penalty: l1 or l2\n",
        "  lr_param_grid = None\n",
        "  # refer to sklearn documentation on grid search and logistic regression\n",
        "  # write your code here...\n",
        "  return lr_param_grid\n",
        "\n",
        "def get_paramgrid_rf():\n",
        "  # you need to return parameter grid dictionary for use in grid search cv\n",
        "  # n_estimators: 1, 10, 100\n",
        "  # criterion: gini, entropy\n",
        "  # maximum depth: 1, 10, None  \n",
        "  rf_param_grid = None\n",
        "  # refer to sklearn documentation on grid search and random forest classifier\n",
        "  # write your code here...\n",
        "  return rf_param_grid\n",
        "\n",
        "def perform_gridsearch_cv_multimetric(model=None, param_grid=None, cv=5, X=None, y=None, metrics=['accuracy','roc_auc']):\n",
        "  \n",
        "  # you need to invoke sklearn grid search cv function\n",
        "  # refer to sklearn documentation\n",
        "  # the cv parameter can change, ie number of folds  \n",
        "  \n",
        "  # metrics = [] the evaluation program can change what metrics to choose\n",
        "  \n",
        "  grid_search_cv = None\n",
        "  # create a grid search cv object\n",
        "  # fit the object on X and y input above\n",
        "  # write your code here...\n",
        "  \n",
        "  # metric of choice will be asked here, refer to the-scoring-parameter-defining-model-evaluation-rules of sklearn documentation\n",
        "  \n",
        "  # refer to cv_results_ dictonary\n",
        "  # return top 1 score for each of the metrics given, in the order given in metrics=... list\n",
        "  \n",
        "  top1_scores = []\n",
        "  \n",
        "  return top1_scores\n",
        "\n",
        "###### PART 3 ######\n",
        "\n",
        "class MyNN(nn.Module):\n",
        "  def __init__(self,inp_dim=64,hid_dim=13,num_classes=10):\n",
        "    super(MyNN,self)\n",
        "    \n",
        "    self.fc_encoder = None # write your code inp_dim to hid_dim mapper\n",
        "    self.fc_decoder = None # write your code hid_dim to inp_dim mapper\n",
        "    self.fc_classifier = None # write your code to map hid_dim to num_classes\n",
        "    \n",
        "    self.relu = None #write your code - relu object\n",
        "    self.softmax = None #write your code - softmax object\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x = None # write your code - flatten x\n",
        "    x_enc = self.fc_encoder(x)\n",
        "    x_enc = self.relu(x_enc)\n",
        "    \n",
        "    y_pred = self.fc_classifier(x_enc)\n",
        "    y_pred = self.softmax(y_pred)\n",
        "    \n",
        "    x_dec = self.fc_decoder(x_enc)\n",
        "    \n",
        "    return y_pred, x_dec\n",
        "  \n",
        "  # This a multi component loss function - lc1 for class prediction loss and lc2 for auto-encoding loss\n",
        "  def loss_fn(self,x,yground,y_pred,xencdec):\n",
        "    \n",
        "    # class prediction loss\n",
        "    # yground needs to be one hot encoded - write your code\n",
        "    lc1 = None # write your code for cross entropy between yground and y_pred, advised to use torch.mean()\n",
        "    \n",
        "    # auto encoding loss\n",
        "    lc2 = torch.mean((x - xencdec)**2)\n",
        "    \n",
        "    lval = lc1 + lc2\n",
        "    \n",
        "    return lval\n",
        "    \n",
        "def get_mynn(inp_dim=64,hid_dim=13,num_classes=10):\n",
        "  mynn = MyNN(inp_dim,hid_dim,num_classes)\n",
        "  mynn.double()\n",
        "  return mynn\n",
        "\n",
        "def get_mnist_tensor():\n",
        "  # download sklearn mnist\n",
        "  # convert to tensor\n",
        "  X, y = None, None\n",
        "  # write your code\n",
        "  return X,y\n",
        "\n",
        "def get_loss_on_single_point(mynn=None,x0,y0):\n",
        "  y_pred, xencdec = mynn(x0)\n",
        "  lossval = mynn.loss_fn(x0,y0,y_pred,xencdec)\n",
        "  # the lossval should have grad_fn attribute set\n",
        "  return lossval\n",
        "\n",
        "def train_combined_encdec_predictor(mynn=None,X,y, epochs=11):\n",
        "  # X, y are provided as tensor\n",
        "  # perform training on the entire data set (no batches etc.)\n",
        "  # for each epoch, update weights\n",
        "  \n",
        "  optimizer = optim.SGD(mynn.parameters(), lr=0.01)\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    ypred, Xencdec = mynn(X)\n",
        "    lval = mynn.loss_fn(X,y,ypred,Xencdec)\n",
        "    lval.backward()\n",
        "    optimzer.step()\n",
        "    \n",
        "  return mynn\n",
        " "
      ],
      "metadata": {
        "id": "YxxeQiUodL7O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}